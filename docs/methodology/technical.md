# Technical Realisation

!!!abstract
    The model uses GAMS for data transformations and model generation and applies the industry LP and MIP solver CPLEX for solution. The code adheres to strict coding guidelines, for instance with regard to naming conventions, code structuring and documentation, including a modular approach. A set of carefully chosen compilation and exploitation tests is used to check the code. The code is steered by a GUI based on GGIG (ref., Java code) which also support result exploitation.

## Overview of the Technical Realisation


The model template and the coefficient generator are realised in GAMS
(General Algebraic Modeling System), a widely used modeling language
for economic simulation models. GAMS is declarative (as seen from the
template discussion above), i.e. the structure of the model's equation
is declared once, and from there different model instances can be
generated. GAMS supports scripting for data transformation, extensively
used by the coefficient generator and by the post-model reporting.

![](../media/Figure11.png)
:   Figure 11: Overview of technical realisation.
    Source: Own illustration

Additionally, as an extension of the experiment exploiter, *machine
learning* (for detailed description see Britz, 2011) can
be used to derive correlations and dependencies between model results
and available model variables.

## MIP Solution Strategy


In opposition to purely linear problems, Mixed-Integer problem models
(MIPs) are far harder to solve. In order to find the optimum, in theory
the combinatorial set of all binaries respectively general integer
variables would need to be evaluated. Depending on the simulation
horizon of FarmDyn, the number of farm branches considered and the time
resolution for investment and labour use decisions, a model instance can
comprise between a few dozens to more than a thousand binary variables,
with often several ten thousand variables and equations in total.

There are huge differences in the quality of LP and more so MIP solvers.
Industry solvers such as CPLEX or GUROBI reflect continuous investments
into algorithmic improvements over decades. Fortunately, both offer free
academic licenses. The code is set-up to work with both solvers to be
secured should license conditions change as well as switch in cases one
of the solvers outperforms considerably the other. Current tests seem to
show a slight advantage for CPLEX. Both solvers can benefit from
parallel processing. Model instances should therefore if possible be
solved on a multi-core computing server. The option files for the
solvers are currently defined such that one core is not used by the
program and left free for other processing load.

The relaxed version of the model (where binaries and integers are
removed and treated as continuous variables) can typically be solved in
a few seconds, and once such a starting point is given, slight
modifications to the model take very little time to solve despite the
model size. However, regardless of tremendous algorithmic improvements
in solving MIPs, the MIP version could take quite long to solve without
some solution tactic.

The model code therefore integrates different strategies to speed up the
solution process for the MIP. Some of those are generally applicable to
MIP problems, typically offered by GAMS and/or the MIP solvers, others
follow tactics proposed to speed up the solution time of MIP problems,
but require a specific implementation reflecting the model structure. In
the following, these strategies are roughly described, starting first
with the model generic.

In order to define a lower bound on the objective which allows the
solver to cut-off parts of the tree, the model is first solved in
relaxed mode (RMIP) with the farm switched off such that income can only be
generated by working off-farm (*v\_hasFarm* is fixed to zero). Solving
that variant takes less than a second. The solution is used to define
the lower cut-off for MIP solver. Next, the model is solved as RMIP with
only one SON, and afterwards, the state contingent variables
are copied to all other SON, before the RMIP is solved
again. The main statements are given in the *exp\_starter.gms* file.

The relaxed (RMIP) solution defines the upper cut-off -- forcing certain
variables to only take on integer values can only reduce the objective
function. At the same time, it proves a basis for solving the MIP.
However, in many instances it has not proven useful to use the solution
of RMIP as MIP start starting point, both CPLEX and GUROBI seem to spend
considerable time to construct a feasible integer solution from the RMIP
solution.

As stated above, solving a MIP problem to its true optimum can be
tremendously time consuming. Therefore, typically MIP problems are only
solved given an optimality tolerance. The branch-and-cut algorithm used
in MIP solvers always provide a safe upper limit for the objective value
stemming from a relaxed version of the current tree node. Accordingly,
they can quantify the maximal absolute and relative gap to the
potentially maximal objective function. Typically, the smaller the
desired gap, the larger the number of combination of integer variables
the solver needs to test. Forcing the gap to zero requires more or less
a test of all combination, i.e. ten-thousands of solves of a LP version
of the model with binaries and integers fixed. In most production runs,
a relative gap of 0.5% has proven as acceptable. The solver will then
stop further search for a better solution once a MIP solution has been
found which differs by less from the relaxed best node.

The problem with the gap is clearly that differences between two
simulations can not only stem from different model inputs (prices,
policy etc.), but also simply from the fact that the gap at the best
solutions returned by the solver for each run differs.

MIP solvers can also "tune" their options based on one or several given
model instance. Tuning is available both with CPLEX and GUROBI, and can
be switched on via the interface. That process takes quite long, as the
model is repeatedly solved with different solver options. The parameters
from the tuning step are stored in an option and can be used by
subsequent runs.

### Fractional investments of machinery

An option to reduce the number of binaries is to treat certain
investment decisions as continuous. For machinery, the model allows to
replace the binary variable *v\_buyMach* by a fractional replacement
*v\_buyMachFlex*. The replacement depends on a threshold for the
depreciation costs per ha or hour, which can be set by the interface.
The larger the threshold, the lower is the number of integer variables
and the higher the (potential) difference to the solution where more
indivisibilities in machine investments are taken into account.

The relevant code section (*define_starting_bounds.gms*) is shown below:

[embedmd]:# (N:/em/work1/FarmDyn/FarmDyn_QM/gams/model/define_starting_bounds.gms GAMS /\$ifi.*?dynamics/ /;/)
```GAMS
$ifi "%dynamics%" == "comparative-static"  $setglobal buyMachFlexThreshold 1E+6
$ifi not setglobal buyMachFlexThreshold $setglobal buyMachFlexThreshold 3


    v_buyMach.fx(machType,t,nCur)     $ (t_n(t,nCur)
    $ (    (p_machAttr(machType,"depCost_ha")    le %buyMachFlexThreshold%) $ p_machAttr(machType,"depCost_ha")
        or (p_machAttr(machType,"depCost_hour")   le %buyMachFlexThreshold%) $ p_machAttr(machType,"depCost_hour")
                                     ) $ (not p_machAttr(machType,"years"))) = 0;
```

[embedmd]:# (N:/em/work1/FarmDyn/FarmDyn_QM/gams/model/define_starting_bounds.gms GAMS /v_buyMachFlex\.fx/ /;/)
```GAMS
v_buyMachFlex.fx(machType,t,nCur) $ (t_n(t,nCur)
    $  (   (p_machAttr(machType,"depCost_ha")    gt %buyMachFlexThreshold%)
        or (p_machAttr(machType,"depCost_hour")  gt %buyMachFlexThreshold%)
                                         or p_machAttr(machType,"years"))) = 0;
```

### Heuristic reduction of binaries

On demand, the RMIP solution can be used in combination with some
heuristic rules to reduce the set of endogenous variables. As the RMIP
solution will e.g. build a fraction of larger stables and thus save
costs compared to the MIP solution, the herd size in the MIP solution
can be assumed to be upper bounded by the solution of the MIP.
Similarly, as investment costs for machinery will be underestimated by
the MIP, it can be assumed that machinery not bought in the RMIP
solution will not be found in the optimal solution of the MIP.

An example is shown below for investment decision into stables. The
program first defines the maximal amount of stable places used in any
year. Investments into stables and their usage which are larger than the
maximal size or smaller than 2/3 of the maximal size are removed from
the MIP. Equally, investment in stables is set to zero if there was no
investment in the RMIP solution.

[embedmd]:# (N:/em/work1/FarmDyn/FarmDyn_QM/gams/model/reduce_vars_for_mip.gms GAMS /\*.*?bigger/ /\*.*?MIP.*?RMIP/)
```GAMS
* --- exclude bigger stable investment under binary conditions
*
  p_maxBoughtStableSize(stableTypes,leaves)
     = smax(t_n(tCur,nCur) $ sameScen(leaves,nCur),
                       sum( (stables1) $ p_stableSize(stables1,stableTypes),
                                        v_buyStables.l(stables1,"long",tCur,nCur)*p_stableSize(stables1,stableTypes)))
           $ (smax(t_n(tCur,nCur) $ sameScen(leaves,nCur),
                        sum( (stables1) $ p_stableSize(stables1,stableTypes),
                                 v_buyStables.l(stables1,"long",tCur,nCur)*p_stableSize(stables1,stableTypes))) ne -INF);



  p_maxInvStableSize(stableTypes,leaves)
     = max(smax(t_n(tCur,nCur) $ sameScen(leaves,nCur),
                       sum(stables1 $ p_stableSize(stables1,stableTypes),
                            v_stableUsed.l(stables1,tCur,nCur)*p_stableSize(stables1,stableTypes)))
           $ (smax(t_n(tCur,nCur) $ sameScen(leaves,nCur),
                        sum(stables1 $ p_stableSize(stables1,stableTypes),
                           v_stableUsed.l(stables1,tCur,nCur)*p_stableSize(stables1,stableTypes))) ne -INF),

           smax(tOld,
                       sum(stables1 $ p_stableSize(stables1,stableTypes),
                         p_iniStables(stables1,"long",tOld)*p_stableSize(stables1,stableTypes)))
         $  (smax(tOld,
                       sum(stables1 $ p_stableSize(stables1,stableTypes),
                         p_iniStables(stables1,"long",tOld)*p_stableSize(stables1,stableTypes))) ne -INF));

  p_maxBoughtStableSize(stableTypes,leaves)
     = smin(stables1 $ (p_stableSize(stables1,stableTypes) ge p_maxBoughtStableSize(stableTypes,leaves)),
               p_stableSize(stables1,stableTypes));

  p_maxBoughtStableSize(stableTypes,leaves) $ (p_maxBoughtStableSize(stableTypes,leaves) eq INF)
     = smax(stables1 $ p_stableSize(stables1,stableTypes),p_stableSize(stables1,stableTypes));


  p_maxInvStableSize(stableTypes,leaves)
     = smin(stables1 $ (p_stableSize(stables1,stableTypes) ge p_maxInvStableSize(stableTypes,leaves)),
               p_stableSize(stables1,stableTypes));

  p_maxInvStableSize(stableTypes,leaves) $ (p_maxInvStableSize(stableTypes,leaves) eq INF)
     = smax(stables1 $ p_stableSize(stables1,stableTypes),p_stableSize(stables1,stableTypes));

  v_stableInv.up(stables,hor,t,nCur)
    $ (t_n(t,nCur) $ sum( (stableTypes,sameScen(nCur,leaves)) $ (     (p_stableSize(stables,stableTypes) le p_maxInvStableSize(stableTypes,leaves))
                         and  (p_stableSize(stables,stableTypes) ge p_maxBoughtStableSize(stableTypes,leaves)*6/10)
                                         $ p_stableSize(stables,stableTypes)),1 )) = 1;

  v_stableInv.lo(stables,hor,t,nCur)
   $ (t_n(t,nCur) $ sum( (stableTypes,sameScen(nCur,leaves)) $ (     (p_stableSize(stables,stableTypes) le p_maxInvStableSize(stableTypes,leaves))
                        and  (p_stableSize(stables,stableTypes) ge p_maxBoughtStableSize(stableTypes,leaves)*6/10)
                                        $ p_stableSize(stables,stableTypes)),1 )) = 0;

  v_stableInv.up(stables,hor,t,nCur)
    $ ( sum( (stableTypes,sameScen(nCur,leaves)) $ ((p_stableSize(stables,stableTypes) gt p_maxInvStableSize(stableTypes,leaves))
             $ p_stableSize(stables,stableTypes)),1 )  ) = 0;

  v_buyStables.up(stables,hor,t,nCur)
    $ (sum((stables1,stableTypes) $ (p_stableSize(stables,stableTypes) $ p_stableSize(stables1,stableTypes)), v_buystables.l(stables1,hor,t,nCur))
     $ sum( (stableTypes,sameScen(nCur,leaves)) $ ((p_stableSize(stables,stableTypes) lt p_maxInvStableSize(stableTypes,leaves))
         $  (p_stableSize(stables,stableTypes) ge p_maxBoughtStableSize(stableTypes,leaves)*6/10)
               $ p_stableSize(stables,stableTypes)),1 ) $ t_n(t,nCur) ) = 1;


   v_stableUsed.up(stables,t,nCur)
    $ ( sum((stableTypes,sameScen(nCur,leaves)) $ ((p_stableSize(stables,stableTypes) gt p_maxInvStableSize(stableTypes,leaves))
                 $ p_stableSize(stables,stableTypes)),1 ) $ t_n(t,nCur)) = 0;

   v_buyStables.up(stables,hor,t,nCur)
    $ ( sum((stableTypes,sameScen(nCur,leaves)) $ ((p_stableSize(stables,stableTypes) gt p_maxInvStableSize(stableTypes,leaves))
                 $ p_stableSize(stables,stableTypes)),1 ) $ t_n(t,nCur)) = 0;

   v_stableInv.up(stables,hor,t_n(t,nCur))
      $ ( sum((stableTypes,sameScen(nCur,leaves)) $ ((p_stableSize(stables,stableTypes) lt p_maxBoughtStableSize(stableTypes,leaves)*5/10)
                  $ p_maxBoughtStableSize(stableTypes,leaves)
                  $ p_stableSize(stables,stableTypes)),1 ) ) = 0;

   v_buyStables.up(stables,hor,t_n(t,nCur))
      $ sum((stableTypes,sameScen(nCur,leaves)) $ ((p_stableSize(stables,stableTypes) lt p_maxBoughtStableSize(stableTypes,leaves)*5/10)
                  $ p_maxBoughtStableSize(stableTypes,leaves)
                  $ p_stableSize(stables,stableTypes)),1 ) = 0;

   v_buyStables.up(stables,hor,t,nCur)
      $ ( sum((stableTypes,sameScen(nCur,leaves)) $ ((p_stableSize(stables,stableTypes) lt p_maxInvStableSize(stableTypes,leaves)*5/10)
                  $ p_stableSize(stables,stableTypes)),1 ) $ t_n(t,nCur)) = 0;

*
*  --- do not buy stables in MIP mode if never bought under RMIP
```


Similar statements are available for investments into manure silos,
buildings and machinery. These heuristics are defined in
"*model\\reduce\_vars\_for\_mip.gms*". It is generally recommended to
use these statements as they can considerably reduce solving time.
However, especially after structural changes to the code, checks should
be done if the rules do not actually prevent the model from finding the
(optimal) MIP solution.

### Binary fixing heuristics

In order to speed up solution, the heuristics discussed above are
coupled with repeated RMIP solves where integer variable from the last
fractional solution are moved to zero or unity depending on the solution
and heuristics rules. To give an example: if parts of machinery are
bought over time such that their sum exceeds a threshold, for instance
half a tractor, the *machBuy* variable in the first year where the
machine is bought is fixed to zero. These pre-solves can lead to start
point for the MIP solves where most integer variables are already no
longer fractional which can speed up solution.

### Equations which support the MIP solution process

Another tactic to ease the solution of MIPs is to define equations,
which decrease the solution space for the integer variables based on the
level of fractional variables respectively defining logical ordering for
the integer decisions. These equations are not necessarily truly
restricting the solution space, they only reinforce existing relations
between variables. The additional equations often reduce the overall
solution time by improving the branching more than by increasing single
LP iterations due to the increase in the constraints.

One way to improve the branching order is to link binaries with regard to dynamics. There are currently *three ordering equations over time*. The first two prescribes respectively that if a farm has a cow herd in t+1 this implies that a cow herd in the previous year existed:

[embedmd]:# (N:/em/work1/FarmDyn/FarmDyn_QM/gams/model/cattle_module.gms GAMS /hasHerdOrderDairy_.*?\.\./ /;/)
```GAMS
hasHerdOrderDairy_(tCur(t),nCur) $ (tCur(t-1) $ t_n(t,nCur)) ..

       v_HasBranch("dairy",t,nCur) =L= sum(t_n(t-1,nCur1) $ anc(nCur,nCur1), v_hasBranch("dairy",t-1,nCur1));
```

[embedmd]:# (N:/em/work1/FarmDyn/FarmDyn_QM/gams/model/templ.gms GAMS /hasFarmOrder_.*?\.\./ /;/)
```GAMS
hasFarmOrder_(tCur(t),nCur) $ (tCur(t-1) $ t_n(t,nCur)) ..

       v_hasFarm(t,nCur) =L= sum(t_n(t-1,nCur1) $ anc(nCur,nCur1), v_hasFarm(t-1,nCur1));
```

The third one implies that working off-farm in a year t implies also
working off-farm afterwards:

[embedmd]:# (N:/em/work1/FarmDyn/FarmDyn_QM/gams/model/templ.gms GAMS /workOrder_.*?\.\./ /;/)
```GAMS
workOrder_(tCur(t),nCur) $ ((sum(workOpps(workType) $ (v_labOff.up(t,nCur,workType) ne 0),1) $ tCur(t-1)) $ t_n(t,nCur)) ..

       v_labOffB(t,nCur) =G= sum(t_n(t-1,nCur1) $ anc(nCur,nCur1), v_labOffB(t-1,nCur1));
```

Another tactic followed is to define logical high level binaries which
dominate other. These *general binaries* are partly already shown above:
the *v\_hasFarm* and *v\_workOffB* variables. The later one is linked to
the individual off-farm working possibilities:

[embedmd]:# (N:/em/work1/FarmDyn/FarmDyn_QM/gams/model/templ.gms GAMS /convLab_.*?\.\./ /;/)
```GAMS
convLab_(tCur(t),nCur) $ (sum(workOpps(workType)$ (v_labOff.up(t,nCur,workType) ne 0),1) $ t_n(t,nCur) ) ..

       sum(workOpps(workType), v_labOff(t,nCur,workType)) =E= v_labOffB(t,nCur);
```

In order to support the solving process, *w\_workOff* is defined as a
*SOS1* variable, which implies that at most one of the *workType*
options is greater than zero in any year.

The *v\_hasFarm* variables dominates the *v\_hasBranch* variables:

[embedmd]:# (N:/em/work1/FarmDyn/FarmDyn_QM/gams/model/templ.gms GAMS /hasFarmOrder_.*?\.\./ /;/)
```GAMS
hasFarmOrder_(tCur(t),nCur) $ (tCur(t-1) $ t_n(t,nCur)) ..

       v_hasFarm(t,nCur) =L= sum(t_n(t-1,nCur1) $ anc(nCur,nCur1), v_hasFarm(t-1,nCur1));
```

That equation is additionally linked to the logic of the model as
*v\_hasFarm* implies working hours for general farm management.

Furthermore, general binary exists which controls if a herd is present
in any year, *v\_hasAlwaysHerd*. If it is switched on, it will imply a
dairy herd in any year.This is based on the equation *hasAlwaysLast\_*
together with the order equation *hasHerdOrder\_* shown below.

![](../media/image244.png)
-> gibt es so nicht mehr

The equations which support the MIP solution process by linking
fractional variables to binary ones relate to investment decisions.
Firstly, investments in machinery are only possible if there is matching
machinery need:

[embedmd]:# (N:/em/work1/FarmDyn/FarmDyn_QM/gams/model/templ.gms GAMS /machBuy_.*?nCur/ /;/)
```GAMS
machBuy_(curMachines(machType),machLifeUnit,t,nCur)
          $ (     (v_machInv.up(machType,machLifeUnit,t,nCur) ne 0)
                $ (v_buyMach.up(machType,t,nCur) ne 0)
                $ p_lifeTimeM(machType,machLifeUnit)  $ p_priceMach(machType,t)
                $ (not sameas(machLifeUnit,"years")) $ t_n(t,nCur)  )  ..
          v_buyMach(machType,t,nCur)

              =L= (v_machNeed(machType,machLifeUnit,t,nCur) $ tCur(t)
*
*        --- minus operating hours of weighted average over normal planning period
*            if beyond the normal planning period
*
                   + [sum( (t_n(t1,nCur1)) $ ( (p_year(t1) lt p_year(t)) $ tCur(t1) $ isNodeBefore(nCur,nCur1)),
                                                 v_machNeed(machType,machLifeUnit,t1,nCur1)
                                                                     * 1/(p_year(t)+5 - p_year(t1)) )
                     /sum( (t1) $ ( (p_year(t1) lt p_year(t)) $ tCur(t1)),    1/(p_year(t)+5 - p_year(t1)) )
                      ] $ ( (not tCur(t)) and p_prolongCalc)

                   ) * 10;
```

Secondly, two equations link the dairy
herd to investment decisions into stables and manure storage silos:

[embedmd]:# (N:/em/work1/FarmDyn/FarmDyn_QM/gams/model/general_herd_module.gms GAMS /stableBuy_.*?\.\./ /;/)
```GAMS
stableBuy_(stables,hor,tCur(t),nCur) $ ( (v_buyStables.up(stables,hor,t,nCur) gt 0) $ t_n(t,nCur)) ..

       v_buyStables(stables,hor,t,nCur) =L= sum(stableTypes_to_branches(stableTypes,branches)
                                              $ p_stableSize(stables,stableTypes), v_HasBranch(branches,t,nCur));
```

[embedmd]:# (N:/em/work1/FarmDyn/FarmDyn_QM/gams/model/general_herd_module.gms GAMS /stableInvb_.*?nCur/ /;/)
```GAMS
stableInvb_(stables,hor,tCur(t),nCur)
        $ ( (    (sum( t_n(t1,nCur1) $ (isNodeBefore(nCur,nCur1) or sameas(nCur,nCur1)),
                             v_buyStables.up(stables,hor,t1,nCur1)) gt 0)
              or (sum( tOld, p_iniStables(stables,hor,tOld)))) $ t_n(t,nCur) ) ..

       v_stableInv(stables,hor,t,nCur)  =L= sum(stableTypes_to_branches(stableTypes,branches)
                                              $ p_stableSize(stables,stableTypes), v_HasBranch(branches,t,nCur));
```

These supporting restrictions can be switched off from the model via the
interface, to check if they unnecessarily restrict the solution domain
of the solver. It is generally recommended to use them as they have
proven to speed up the solution process.

### Priorities

Finally, there are options to help the MIP solver to decide which
branches to explore first. The variable field .prior in GAMS allows
setting priorities which are passed to the MIP solver; lower priorities
are interpreted as having precedence. The file "model\\def\_priors.gms"
defines such priorities.

The model is instructed to branch first on the decision to have a herd
in any year, next on having a farm and the individual branches:

[embedmd]:# (N:/em/work1/FarmDyn/FarmDyn_QM/gams/model/def_priors.gms GAMS / v_hasAlwaysHerd\.prior/ /;/)
```GAMS
 v_hasAlwaysHerd.prior                           = %priorOperator%  (p_priorMax*20);
```

[embedmd]:# (N:/em/work1/FarmDyn/FarmDyn_QM/gams/model/def_priors.gms GAMS / v_hasFarm\.prior/ /;/)
```GAMS
 v_hasFarm.prior(t,n)        $ t_n(t,n)          = %priorOperator%  (p_priorMax*6  +  %timeWeight%);
```

[embedmd]:# (N:/em/work1/FarmDyn/FarmDyn_QM/gams/model/def_priors.gms GAMS / v_hasBranch\.prior/ /;/)
```GAMS
 v_hasBranch.prior("dairy",t,n) $ t_n(t,n) = %priorOperator%  (p_priorMax*4  +  %timeWeight%);
```

[embedmd]:# (N:/em/work1/FarmDyn/FarmDyn_QM/gams/model/def_priors.gms GAMS / v_hasBranch\.prior.*?farm/ /;/)
```GAMS
 v_hasBranch.prior("farm",t,n)   $ t_n(t,n)         = %priorOperator%  (p_priorMax*5  +  %timeWeight%);
```

Generally, early years are given precedence:

[embedmd]:# (N:/em/work1/FarmDyn/FarmDyn_QM/gams/model/def_priors.gms GAMS /\$setglobal timeWeigh/ /10/)
```GAMS
$setglobal timeWeight (card(t)-ord(t)+1)/card(t)  * p_priorMax * 10
```

The *p\_priorMax* is the maximal priorities assigned to stables, which
is defined by a heuristic rule: large stables are tried before smaller
ones, cow stable before young cattle and calves stables, and finally
long-term investment in the whole building done before maintenance
investments:

[embedmd]:# (N:/em/work1/FarmDyn/FarmDyn_QM/gams/model/def_priors.gms GAMS /parameter p_priorStables/ /1;/)
```GAMS
parameter p_priorStables(stables);

       p_priorStables(stables) $ sum(stableTypes $ p_stableSize(stables,stableTypes), stableTypes.pos)
          = sqr(1/sum(stableTypes $ p_stableSize(stables,stableTypes), stableTypes.pos)*10)

                                  * sqrt( sum(stableTypes $ p_stableSize(stables,stableTypes),
                                                      p_stableSize(stables,stableTypes))
                                       / smax((stables1,stableTypes) $ p_stableSize(stables,stableTypes),
                                                 p_stableSize(stables1,stableTypes)));

       p_priorMax              = smax(stables, p_priorStables(stables)) * card(hor);
       p_priorStables(stables) = p_priorStables(stables)/p_priorMax;
       p_priorMin              = smin(stables, p_priorStables(stables));
       p_priorMax              = 1;
```

Off-farm work decisions currently receive a lower priority compared to
investments into stables:

[embedmd]:# (N:/em/work1/FarmDyn/FarmDyn_QM/gams/model/def_priors.gms GAMS /v_buyStables\.prior/ /;/)
```GAMS
v_buyStables.prior(stables,hor,t,n) $ t_n(t,n)  = %priorOperator%  (p_priorStables(stables)*hor.pos + %timeWeight%);
```

[embedmd]:# (N:/em/work1/FarmDyn/FarmDyn_QM/gams/model/def_priors.gms GAMS /v_stableInv\.prior/ /;/)
```GAMS
v_stableInv.prior(stables,hor,t,n)  $ t_n(t,n)  = %priorOperator%  [(p_priorStables(stables)*hor.pos + %timeWeight%)*0.95];
```

[embedmd]:# (N:/em/work1/FarmDyn/FarmDyn_QM/gams/model/def_priors.gms GAMS /v_labOffB\.prior/ /;/)
```GAMS
v_labOffB.prior(t,n)            $ t_n(t,n)         = %priorOperator%  (p_priorMin * 0.9 +  %timeWeight%);
```

[embedmd]:# (N:/em/work1/FarmDyn/FarmDyn_QM/gams/model/def_priors.gms GAMS /v_labOff\.prior/ /;/)
```GAMS
v_labOff.prior(t,n,workType)    $ t_n(t,n)         = %priorOperator%  (p_priorMin * 0.8 +  %timeWeight%);
```

For other investment decisions, the investment sum is used for priority
ordering, e.g:

[embedmd]:# (N:/em/work1/FarmDyn/FarmDyn_QM/gams/model/def_priors.gms GAMS /p_rank\(buildings/ /;/)
```GAMS
p_rank(buildings) = p_priceBuild(buildings,"%firstYear%") / ( p_building(buildings,"lifeTime") + 15 $ (not p_building(buildings,"lifeTime")));
```

The SOS1 variables should have all the same priorities. Therefore, no
distinction is introduced for the *v\_workOff* and *v\_siCovComb* variables, with the exemption of the time dimension.

Generally, it is recommend using these priorities as they have proven to
speed up the solution process.

## Reporting


As discussed in the following chapter, a GUI allows exploitation of
model results, also comparing different model runs. That part requires
that all results are stored in one multi-dimensional cube. Accordingly,
after the model is solved, its variables are copied to a result
parameter, as shown in the following example:

[embedmd]:# (N:/em/work1/FarmDyn/FarmDyn_QM/gams/exploiter/store_res.gms GAMS / p_res.*?liquid/ /;/)
```GAMS
 p_res(%1,%2,"liquid","sum","",tCur)       = sum(t_n(tCur,nCur), p_probn(nCur) * v_liquid.l(tCur,nCur));
```

[embedmd]:# (N:/em/work1/FarmDyn/FarmDyn_QM/gams/exploiter/store_res.gms GAMS / p_res.*?liquid.*?mean/ /;/)
```GAMS
 p_res(%1,%2,"liquid","sum","","mean")     = sum(t_n(tCur,nCur), p_probn(nCur) *v_liquid.l(tCur,nCur))/p_cardTCur;
```

## Systematic sensitivity analysis based on Design of Experiments


As discussed above, solution for one indicator and one GHG emission
target might require between a few seconds to several minutes on a
powerful multi-core machine. The derivation of the marginal abatement
cost curves requires solving repeatedly model instances over a range of
GHG emission targets, therefore it might require an hour or more to
solve one specific farm configuration.

An application of the model to a larger sample of existing farms is
consequently computationally impossible. This is why it was envisaged
from the beginning to use sensitivity analysis to generate a sufficient
number of instances to derive a meta-model in order to estimate
abatement costs for larger population of farms, for example based on an
appropriate regression model.. Meta modeling seems also a suitable tool
to learn more about which farm attribute impact abatement costs and to
which extend the occurring Marginal Abatement Costs (MACs) depend on the GHG calculation procedure
of the different indicators.

For this four steps are required:

1.  Setting up of appropriate sensitivity experiments which cover the
    distribution of farm attributes in an appropriate sample (such as
    the farm structure survey for North-Rhine-Westphalia). Consequently,
    this requires the use of an efficient and space filling random
    sampling design to lower the necessary sample size for the
    derivation of a meta-model. At the same time, it has to be ensured
    that the randomised factor level combinations are smoothly
    distributed over the range of factor level permutations. [^13]

2.  Running the single farm model on these experiments and collecting
    key results.

3.  Deriving a meta model from these experiments.

This section focuses mainly on technical aspects of this process.

The overall strategy consists of combining a Java based package for
interface generation and result exploitation, which also comprises a
machine learning package, with GAMS code. For the definition of
representative sensitivity experiments a sampling routine, (lhs\_0.10)
implemented in R (version 2.15.1) is combined with the GAMS code to
generate sample farms under recognition of correlations between factors.

The GAMS code *(scen\_gen*) modifies settings entered via the interface
(see next section) to define attributes for each experiment. A single
farm run is then executed as a child process with these settings. The
user is able to define upper and lower bounds for single factors to
define a solution room in which factor levels can vary between scenarios
for different production specific attributes of the farm (see next
section). The interface also allows defining if correlations between
selected variables should be recognized during the sample randomization
procedure. Furthermore, depending on the number of draws and the
complexity of the assumed correlation matrix a maximum number of
sampling repetitions can be selected [^14].

Only the factors for which the selected maximum value differs from the
minimum value are varied between model runs. Hence, the user is able to
fix factor levels for single factors over all experiments by defining
the minimum and maximum factor level. The upper and lower bounds of the
variables define the solution space of possible factor level
combinations of different factors. If the chosen minimum and maximum
values are equal, the factor level of the specific attribute is holding
constant during the scenario definitions. For the definition of wage
rates and prices for concentrates the user is able to select constant
differences to the full time wage rate or the concentrate type 1.

With increasing number of factors that can vary between scenarios and
increasing possible factor levels per factor, the number of possible
scenarios (factor level permutations) will increase exponentially (up to
a few thousands). Hence, to create model outputs representative for all
admissible scenarios, a large number of scenario runs would have to be
processed to get reliable outputs for the derivation of a meta-model.

As this would cause long computing time also on a multi-core processor
(several days), the numbers of scenario runs have to be restricted to a
manageable number, while at the same time being representative for the
real life distribution of farm attributes.

Therefore, the scenario definition is done by Latin Hypercube Sampling
(LHS) to create an efficient sample with a small sample size (to lower
computing time) while guaranteeing a space filling sample design over
the full range of admissible scenarios (McKay et al. 1979, Iman and
Conover 1980). This is done, using a bridge from GAMS to the statistical
software R. Therefore the LHS package of R has to be installed for being
able to create LHS samples for a defined number of draws *n* and factors
*k* (in our case taking the command "*improvedLHS(n,k*)"). LHS sampling
creates a sample matrix of size n\*k incorporating random values between
0 and 1, which are interpretable as percentages. These are drawn
assuming an uniform distribution between 0 and 1. Further on, LHS
sampling outputs ensure orthogonality of the output matrix and that
factor level combinations evenly distributed over the possible
permutation area.

The GAMS side of the technical implementation is shown in the following:

[embedmd]:# (N:/em/work1/FarmDyn/FarmDyn_QM/gams/scen_gen.gms GAMS /\* Use R to / /putclose;/)
```GAMS
* Use R to define the DOE
*
*------------------------------------------------------------------------------
*
 file rIncFile / "%curDir%/rBridge/incFile.r" /;
 put rIncFile;
 $$setglobal outputFileD  "%scrdir%/fromR"
 $$setglobal inputFile    "%scrdirR%/toR.gdx"

 put ' plotFile    <- "%resdirR%/scenGen/lhs_%scenDes%.pdf"; '/;
 put ' outputFile  <- "%outputFile%"; '/;
 put ' inputFile   <- "%inputFile%"; '/;
 put ' useCorr     <- "%useCorr%"; '/;
 put ' useColors   <- "true"; '/;
 put ' maxRunTime  <- %maxRunTime%; '/;
    putclose;
```

The maximal run time for finding a sample can be defined, *maxRunTime.*
If correlations between variables are known and should be recognised
within the sampling procedure, the command *useCorr* has to be set to
*"true"*. Then the correlation matrix can be defined specifically.

[embedmd]:# (N:/em/work1/FarmDyn/FarmDyn_QM/gams/scen_gen.gms GAMS /\*.*?set correlation/ /;/)
```GAMS
*    --- set correlation matrix
*
*        correlation coefficients are derived from data collections from AMI for prices and
*        from LWK-NRW (Milchviehreport NRW, verschiedene Jahrg�nge, 2007 bis 2011)as well as
*        a data collection of the LKV-NRW in 2012 for 5000 dairy farms in NRW. Correlation between
*        nCows and CowsPerAK stem from the Forschungsdatenzentrum des Bundes und der L�nder after
*        analysis on the "Landwirtschaftsz�hlung 2010", results were aligned with results derived
*        from KTBL (2010,p.541).

     table p_cor1(*,*)

                          WinterCerePrice SummerCerePrice MaizCornPrice WinterRapePrice SummerBeansPrice SummerPeasPrice PotatoesPrice SugarBeetPrice
     WinterCerePrice                            0.8            0.7            0.5            0.5                0.5          0.5             0.5
     SummerCerePrice                                           0.7            0.5            0.5                0.5          0.5             0.5
     MaizCornPrice                                                            0.5            0.5                0.5          0.5             0.5
     WinterRapePrice                                                                         0.5                0.5          0.5             0.5
     SummerBeansPrice                                                                                           0.7          0.5             0.5
     SummerPeasPrice                                                                                                         0.5             0.5
     PotatoesPrice                                                                                                                           0.5
     SugarBeetPrice

     ;
```

The names of the set of varying factors, the factor names, the scenario
name, the desired number of draws and, if activated, also the
correlation matrix are send to R. Then the R file "*rbridge\\lhs.r*" is
executed.

[embedmd]:# (N:/em/work1/FarmDyn/FarmDyn_QM/gams/scen_gen.gms GAMS /set factor_name/ /display p_testDoe;/)
```GAMS
set factor_name(*,*) / name.factors /;
     set scen_name(*,*) / name."%scendes%"/;

     execute_unload "%inputFile%" p_n,factor_name,scen_name,factors,p_cor;
     $$setglobal rFile "%curDir%/rbridge/lhs.r"

     $$if exist "%outputFileD%_doe.gdx" execute "rm %outputFileD%_doe.gdx"
     $$batinclude 'util/title.gms' "'execute %rexe% %rFile%'";
     $$if exist %rexe% execute "%rexe% %rFile% %curDir%/rBridge/incFile.r";

$endif.onlyCollect

*
* --- read output from LHS sampling provided by R
*
 parameter p_doe(*,*);
 execute_load "%outputFile%_doe" p_doe;
 if ( card(p_doe) eq 0, abort "Error generating doe, no data found";);
 display p_doe;

 parameter p_testDoe "Check for mean of draws";
 p_testDoe(factors) = sum(draws, p_doe(draws,factors))/card(draws);
 display p_testDoe;
```


The R-bridge is hence activated (R side). Therefore, several packages are
installed in R from the R library to be able to do LHS sampling:

[embedmd]:# (N:/em/work1/FarmDyn/FarmDyn_QM/gams/rbridge/lhs.r R /#install/ /library\(reshape2\);/)
```R
#install.packages("d:\r\R-2.15.1\library\mc2d_0.1-13.zip",repos=NULL);
#install.packages("d:\r\R-2.15.1\library\mvtnorm_0.9-9992.zip",repos=NULL);
#install.packages("d:\r\R-2.15.1\library\lhs_0.10.zip",repos=NULL);
#install.packages("d:\\temp\\gclus_1.3.1.zip",repos=NULL);
# install.packages("t:\\britz\\gdxrrw_0.0-2.zip",repos=NULL);
# install.packages("D:\\temp\\gdxrrw_1.0.2.zip", repos = NULL, type="source");
 library(lhs);
 library(gdxrrw);
 igdx("N:/soft/gams24.7new/24.7");
 library(mc2d);
 library(mvtnorm);
 library(Matrix);
 library(gclus);
 library(reshape2);
```

*p\_n* denotes the number of draws defined via the GUI, which is equivalent to the number of scenarios resulting from
the sampling routine. *Sys.getenv(....)* asks for commands or
information given by the environment (for example if correlations have
to be recognised or not).

[embedmd]:# (N:/em/work1/FarmDyn/FarmDyn_QM/gams/rbridge/lhs.r R /#useCorr/ /\)\)/)
```R
#useCorr    <- Sys.getenv("useCorr")
#useColors  <- Sys.getenv("useColors")
#inputFile  <- Sys.getenv("inputFile");
#plotFile   <- Sys.getenv("plotFile")
#outputFile <- Sys.getenv("outputFile");
#maxRunTime <- as.numeric(Sys.getenv("maxRunTime"))
```

We decided to use the "*improvedLHS"* type for randomisation [^15] which
produces a sample matrix of *n* rows and *k* columns (n = number of
draws, k = number of factors). This leads to a quite efficient sample
generation in R:

[embedmd]:# (N:/em/work1/FarmDyn/FarmDyn_QM/gams/rbridge/lhs.r R /out1.*?imp/ /\);/)
```R
out1 <- improvedLHS(n,k);
```

Usually, input variables for sensitivity analysis in computer models are
assumed to be independent from each other (Iman et al., 1981a;b). Also
LHS sampling was designed to create a sample of factor level
combinations for different factors avoiding correlations between factors
in random draws to ensure a space filling output. But, for our purposes,
it is important to incorporate as much information about the
multivariate input distribution as possible to get more realistic sample
scenarios and exclude factor combinations that are rather impossible in
reality. Hence, following Iman and Conover (1982:p.331-332) correlation
structure information among input variables should be recognised within
the sampling process, if available. Otherwise "the theoretical
properties of the statistics formed from the output may no longer be
valid." (Iman and Conover 1982:p.331)

To also incorporate information about dependencies between interesting
variables during the sampling procedure we expanded the sampling method
by an approach of Iman and Conover (1982) designing a swapping algorithm
which shuffles observations for single factors between the draws to
mimic given *k\*k* correlation matrix (therefore the R package *MC2d*
including the routine *cornode* is necessary).

[embedmd]:# (N:/em/work1/FarmDyn/FarmDyn_QM/gams/rbridge/lhs.r R /#.*?load c/ /t\);/)
```R
#   --- load correlation matrix from GAMS
#
    t <- rgdx.param(inputFile,"p_cor",names=c("f1","f2"),compress="true");
    t
    t<-acast(t, f1~f2, value.var="value")
    t<-as.matrix(t);
```

To increase the possibility to randomise a sample which offers a
correlation matrix of factors near the proposed one, the routine allows
to repeat the random sampling of demanded *n* draws (yielding *n*
experiment scenarios) for a maximal given computing time ("*maxRunTime*"
e.g. 300 seconds.). The sample (incorporating *n* draws for *k* factors)
with the smallest mean percentage deviation (*meanDev*) between given
and randomised correlation matrix is then selected and send back to GAMS
as the random sample representing the possible population.
Alternatively, the repetition of *n* draws (*n* x *k* sampling matrix)
will be stopped by a threshold value (*if meanDev \< 1*) for the
deviation between the assumed and the randomised sample correlation
matrix.

[embedmd]:# (N:/em/work1/FarmDyn/FarmDyn_QM/gams/rbridge/lhs.r R /bestFit <\- 1/ /\s}\s\s/)
```R
bestFit <- 10;
    iDraw <- 0;


    while( runTime < maxRunTime ){

       iDraw <- iDraw + 1;

       if ( LHSType == "optimumLHS" ) {
          out1 <- optimumLHS(n,k,2,0.01);

          print("shit");

       } else {
          out1 <- improvedLHS(n,k);
       }
```

[embedmd]:# (N:/em/work1/FarmDyn/FarmDyn_QM/gams/rbridge/lhs.r R /# .*?use c/ /\s}\s\s/)
```R
# --- use cornode to apply Iman & conover 1982 to impose correlation
#     t on the LHS matrix out
#
       out1  <- cornode(out1,target=t)
       c <- cor(out1);

       fit = 0;
       for ( i in 1:k )
            for ( j in 1:k )
                if ( fit < bestFit )

       if ( fit < bestFit )
       {
          out     <-out1;
          bestFit <-fit;

          meanDev = sqrt(fit/k)*100;
       };

       if ( iDraw %% reportDraws == 0){
          curTime <- as.numeric(Sys.time(),units="seconds");
          runTime <- curTime - begTime;
          print(paste(" draw :",iDraw," runTime ",round(runTime)," of ",maxRunTime,"seconds, mean sqrt of squared diff between given corr and best draw: ",round(meanDev,2),"%"));
       }
```

For the case that the correlations between factors are given by the
user, leading to an undefined correlation matrix, the program adjusts
the correlation matrix to the nearest one possible:

[embedmd]:# (N:/em/work1/FarmDyn/FarmDyn_QM/gams/rbridge/lhs.r R /#.*?find/ /mat\);/)
```R
#   --- find nearest positive definite matrix
#

    t1<-nearPD(t);

    t <- as.matrix(t1$mat);
```

As mentioned above the LHS sampling defines random value combinations
between all factors in each single draw. Therefore, uniform distributed
random values between 0 and 1 are drawn. The total set of draws defines
one random sample of n single experiments (factor level combinations (in
this stage of the sampling still between 0 and 1)). The routine
implemented into the LHS-module now tries to find the best fitting
sample which corresponds to the demanded correlation matrix most
properly. Sampling outputs of the LHS draws show efficiency
characteristics, also under recognition of correlations. This means that
the mean of drawn random values is still 0.5 (as LHS draws lie between 0
and 1). And if the number of draws is large enough (greater than 30),
quantiles as well as the mean of the distribution of LHS random values
show that we are still consistent with the assumption of an uniform
distribution function of the random draws (between 0 and 1), as
necessary for efficient LHS outputs, also under recognition of factor
correlations. The best fitting sample with the minimal average
percentage deviation of correlations between defined and randomised
correlation matrix is then selected and stored by the program and
automatically printed as a PDF-document for visualisation. The PDF gives
also information about average percentage bias of the randomised
correlation matrix as well as the number of total draws which define the
number of resulting sample experiments:

![](../media/Figure12.png)
:   Figure 12: Scatter plot matrices for different LHS
    samples. With and without the recognition of factor correlations

On the left hand side one can see the scatter plot matrix without any
correlations between factors. In contrast, a clear difference in sampled
values is visualised by the right hand side matrix. For example, a
correlation between *nCows* and *milkYield* was assumed to be 0.8. The
best fitting matrix lead to the same correlation between these two
factors. The correlation coefficients within brackets are the
correlations predefined by the operator. The values in front of the
brackets are the correlation coefficients fitted by the sampling matrix.
The average mean percentage deviation of the randomised correlation
matrix and the assumed correlation matrix is quantified by 7.34%,
meaning, that on average, the randomised correlations deviate by 7.34%
from the predefined ones. The distribution function in the diagonal
shows, that the sampled values of each factor still ensure a uniform
distribution.

The random values for the scenarios are transformed by GAMS to the real
factor levels following the distribution functions of single variables.
A uniform distribution of factor levels for the relevant variables is
assumed. These are easily to define by the minimal value *a* and the
maximum value *b*. A uniform distribution function can be defined by the
following density function (left graph):

(1) $f(x) = \frac{1}{b-a},a \leq b$

Values below *a*, or above *b* have a probability of 0. The
antiderivative expresses the cumulative distribution function of the
random variable whose values lie within the interval $[0; 1]$ (right
graph):

(2) $f(x) = \frac{x-a}{b-a},a \leq x \leq b$

![](../media/Figure13.png)
:   Figure 13: Density function and cumulative
    distribution function of an uniform distributed variable

From the left hand side density function one can easily derive the right
hand side cumulative distribution function. The *y* value of the
distribution function equals the integral
$\int_a^x f(x)$ below the density function (cumulative
probabilities below x).

The given random values of the R-routine (*F(x)*) enable the allocation
of corresponding factor levels (between *a* and *b*) to this random
percentage values from the cumulative distribution function. A random
cumulative probability value *y* corresponds to the factor level *x*
which lies within the real value domain of the interesting factor. Hence
this random sampling procedure produces random values by transforming
uniform distributed random percentages (between 0 and 1) to factor
levels, which are conform to the assumed distribution function of the
variable. So far a uniform distribution function is assumed for the real
factor levels (this can be adjusted to other functions if, for example, a
known population has to be simulated).

For an assumed uniform distribution function of factor levels this is
done following the formula:

(3) $f(x) \cdot (b-a) + a = x$

The randomised value *y* is transformed to the factor level room
concerning the given distribution function of the factor. Hence, for each
single random draw a value is generated for the interesting variable
corresponding to its assumed probability distribution. If a different
distribution is assumed, formula 3 changes.

In GAMS code the formula (3) has to be applied for each factor to
calculate the sample values whereat *p\_doe(draws,"factor")* is
equivalent to F(x). The random percentage *p\_doe(\*,\*)* has to be
multiplied by the difference between the possible maximum and minimum
value of the factor *(%factorMax% - %factorMin%)*. Afterwards the min
value *(%factorMin%)* has to be added to the product to yield the factor
level *x* for the specific factor and scenario. This is illustrated for
some parameters in the following.

[embedmd]:# (N:/em/work1/FarmDyn/FarmDyn_QM/gams/scen_gen.gms GAMS /\*.*?result r/ /true/)
```GAMS
* --- result related declaration
*
  PARAMETER p_res(*,*,*,*,*,*)
           p_meta;
  set resItems / mac,mean,cows,levl,margArab,margGras,margLand,herdRand,cropRand,ProfitDiff,manExportVol,profit/;


  parameter p_scenParam(draws,allFactors) "Numerical values for the scenario specific items";

*
* --- standard setting for aks
*
  p_scenParam(draws,"Aks") = %aks%;
*
* --- general mapping from DOE to factor ranges as defined on interface
*
  p_scenParam(draws,factors)   = p_doe(draws,factors) * (p_ranges(factors,"max")-p_ranges(factors,"min"))+p_ranges(factors,"min");

$iftheni.obDist %useObsDistr% == true
```

*p\_scenParam(draws,factor)* gives the scenario parameter one factor
defined by the random values given by the LHS sampling routine. The
combination of factor levels of the different factors for one single
draw defines one single sensitivity scenario.

The set *scenItems* defines which settings are (possibly) defined
specificly for each scenario:

[embedmd]:# (N:/em/work1/FarmDyn/FarmDyn_QM/gams/scen_gen.gms GAMS /file sc/ /;/)
```GAMS
file scenFile / "incgen/curScen.gms" /;
```

[embedmd]:# (N:/em/work1/FarmDyn/FarmDyn_QM/gams/scen_gen.gms GAMS /alias\(s/ /;/)
```GAMS
alias(scenItems,allFactors);
```

Nevertheless, correlations between factors are able to be recognised
during the sample generation to avoid factor level combinations within
scenarios that conflict with common statistical knowledge; the model
code enables the user to specifically exclude factor level combinations
that seem to be implausible. For example, high labour input per cow and
low milk yield levels or high numbers of cows per farm and only very low
yielding phenotypes.

![](../media/image267.png)
-> wird nicht mehr genutzt so [RW]

These scenario settings must be stored in a GAMS file which is then
picked up by the child processes. In order to keep the system
extendable, firstly, all settings inputted via the GUI are copied over to the specific scenario:

[embedmd]:# (N:/em/work1/FarmDyn/FarmDyn_QM/gams/scengen/gen_inc_file.gms GAMS /\*.*?copy c/ /gms"/)
```GAMS
*  --- copy content of current scen file into new one
*      via OS command
*
   execute "cp %curDir%/incgen/expinc.gms %curDir%/incgen/curScen.gms"
```

Secondly, the modifications defining the specific sensitivity
experiment, i.e. the scenario, are appended with GAMS file output
commands (see *scenGen\\gen\_inc\_file.gms*):

[embedmd]:# (N:/em/work1/FarmDyn/FarmDyn_QM/gams/scengen/gen_inc_file.gms GAMS /\*.*?put s/ /scenFile;/)
```GAMS
*  --- put statements will append to the new scen file
*      and overwrite standard setting
*
   put scenFile;
```

Finally, the content is copied to a specific scenario input file:

[embedmd]:# (N:/em/work1/FarmDyn/FarmDyn_QM/gams/scengen/gen_inc_file.gms GAMS /put_utility/ /;/)
```GAMS
put_utility batch 'shell' / "cp %curDir%/incgen/curScen.gms %curDir%/incgen/"scen.tl".gms";
```

The code to build and optimise the single farm model is realised
in GAMS and uses CPLEX 12.6 in parallel mode as the MIP solver.
Automatic tuning is used to let CPLEX use appropriate solver setting on
the problem. The model instances are set up in order to avoid any
conflicts with I/O operations to allow for parallel execution.

A single instance has a load of about 1.8 cores on average. In a
multi-core machine it seems promising to execute several such processes
in parallel. That is realised by a GAMS program which starts each model
on its own set of input parameters:

[embedmd]:# (N:/em/work1/FarmDyn/FarmDyn_QM/gams/scen_gen.gms GAMS /\*.*?execute/ /;/)
```GAMS
*     --- execute exp_starter as a seperate program, no wait, program will delete a flag at the end to signal that it is ready
*

      put_utility  batch  'msglog'  / '%GAMSPATH%/gams.exe %CURDIR%/exp_starter.gms --scen='allScen.tl
                           ' --iScen='iLoop:0:0' -maxProcDir=255 -output='allScen.tl'.lst'
                          ' --seed=',uniform(0,1000):0:0,
                          ' -maxProcDir=255 -output='allScen.tl:0'.lst %gamsarg% lo=3'
                          ' --pgmName="'allScen.tl' (',iLoop:0:0,' of ',card(allScen):0:0,')"';
```

The name of the scenario, *allScen.tl* is passed as an argument to the
process which will lead a specific include file comprises the definition
of the scenario.

The GAMS process will use its own commando processors and run
asynchronously to the GAMS thread which has started it. The calling
mother process has to wait until all child processes have terminated.
That is achieved by generating a child process specific flag file before
starting the child process:

[embedmd]:# (N:/em/work1/FarmDyn/FarmDyn_QM/gams/scen_gen.gms GAMS /put_utility batch 's/ /\.lst"';/)
```GAMS
put_utility batch 'shell'    / ' %GAMSPATH%gbin/rm  -f "../results/expFarms/res_',scen.tl,'_until_' p_scenParam(scen,"lastYear"):0:0,'.gdx"';
      put_utility batch 'shell'    / ' %GAMSPATH%gbin/rm  -f "%curdir%/incgen/'scen.tl'.gms"';
      put_utility batch 'shell'    / ' %GAMSPATH%gbin/rm  -f "%curdir%/'scen.tl'.lst"';
```

This flag file will be deleted by the child process when it finalises:

![](../media/image273.png)
-> finde ich so nicht [RW]

A simple DOS script waits until all flags are deleted:

[embedmd]:# (N:/em/work1/FarmDyn/FarmDyn_QM/gams/util/TaskSync.bat GAMS /set \/a/ /\s\)/)
```GAMS
set /a _trys=0
:again
IF %_Mode% EXIST %_FlagFiles% (
  set /a _trys+=1
  if %_trys%.==%_MaxTrys%. goto errorexit
  sleep.exe %_seconds%
  goto again
)
```

Using that set-up would spawn for each scenario a GAMS process which
would then execute all in parallel. The mother process would wait until
all child processes have deleted their flag files before collecting
their results. As several dozen or even hundredth of scenarios might be
executed, that might easily block the machine completely, e.g. by
exceeding the available memory.

It is hence necessary to expand the set-up by a mechanism which ensures
that only a pre-defined number of child processes is active in parallel.
That is established by a second simple DOS script which waits until the
number of flag files drops below a predefined threshold:

[embedmd]:# (N:/em/work1/FarmDyn/FarmDyn_QM/gams/util/TaskSyncNFiles.bat GAMS /set \/a _trys=0/ /\s\)/)
```GAMS
set /a _trys=0
:again

set _count=1

for %%x in (%_FlagFiles%) do set /a _count+=1

REM @echo %_count% %_nFiles% >> d:\temp\test.txt

if %_count% gtr %_nFiles% (

  set /a _trys+=1

  if %_trys%.==%_MaxTrys%. goto errorexit

REM @echo %_trys% %_maxTrys% %_seconds% >> d:\temp\test.txt


  sleep.exe %_seconds%

  goto again

)
```

Finally, the results from individual runs are collected and stored. A
GAMS facility is used to define the name of a GDX file to read at run
time:

[embedmd]:# (N:/em/work1/FarmDyn/FarmDyn_QM/gams/scen_gen.gms GAMS /put_utilities/ /;/)
```GAMS
put_utilities batch 'gdxin' / ' ../results/expFarms/res_',scen.tl,'_until_' p_scenParam(scen,"lastYear"):0:0,'.gdx';
```

We now transformed all MAC estimates which are 0 due to an exit decision
of a farm to be able to select these cases for our meta-modeling
estimation (Heckman two-stage selection, described in the next technical
documentation: "R routine to estimate Heckman two stage regression
procedure on marginal abatement costs of dairy farms, based on large
scale outputs of the model DAIRYDYN" by Britz and Lengers (2012)).

[embedmd]:# (N:/em/work1/FarmDyn/FarmDyn_QM/gams/scen_gen.gms GAMS /\*.*?load t/ /\s\);/)
```GAMS
*        --- load the result
*
              execute_load p_res;
              p_dummy = sleep(.01);
*
*        --- filter out results of interest (so far only macs, avAcs and totACs)
*
         $$ifi "%scentype%"=="MAC"     $include 'scengen/scen_load_res_mac.gms'

         $$ifi "%scentype%"=="PROFITS" $include 'scengen/scen_load_res_profits.gms'
         $$ifi "%scentype%"=="Fertilizer directive" $include 'scengen/scen_load_res_profits.gms'

       );
```


Further on, the scenario specific settings which can be used as
explanatory variables for later regressions are stored, see for example:

[embedmd]:# (N:/em/work1/FarmDyn/FarmDyn_QM/gams/scengen/scen_load_res_mac.gms GAMS /\*.*?add/ /mean"\);/)
```GAMS
*     --- add scen variables to store explanatory vars
*
      p_meta(actInds,redLevl,scenItems,actInds1,scen)
       $ sum(redlevl1, p_res(actInds,redLevl1,"mac",actInds1,"mean")) = p_scenParam(scen,scenItems);

      p_meta(actInds,redLevl,actInds,actInds1,scen)
       $ sum(redlevl1, p_res(actInds,redLevl1,"mac",actInds1,"mean")) = 1;

      p_meta(actInds,redLevl,"redLevl",actInds1,scen)
       $ sum(redlevl1, p_res(actInds,redLevl1,"mac",actInds1,"mean")) =
                                                                         p_res(actInds,redLevl,"redlevl",actInds1,"mean");
```


In a next step, the results are stored in a GDX container

[embedmd]:# (N:/em/work1/FarmDyn/FarmDyn_QM/gams/scen_gen.gms GAMS /\*.*?Store/ /;/)
```GAMS
*   --- Store to disk
*
    execute_unload '../results/scenGen/meta_%scenDes%.gdx' s_meta,p_meta=p_res;
```

The major challenge consists in ensuring that the child processes do not
execute write operation on shared files. In the given example, that
relates to the GAMS listing, the GDX container with the results and the
option files generated by the CPLEX tuning step. For the latter, two
options are available: (1) set up child specific directory, copy the
option files into it and use the *optdir* setting in GAMS, or (2) label
the option files accordingly. That latter option was chosen which
restricts the number of scenarios to 450:

[embedmd]:# (N:/em/work1/FarmDyn/FarmDyn_QM/gams/model/def_run.gms GAMS /\*.*?opt3/ /\$else\.iScen/)
```GAMS
* --- opt3 file will by replaced by ###
*     that allows for 300 parallel threads
*
$iftheni.iScen not "%iScen%"==""

$evalGlobal op3 round(%iScen%+100)
$evalGlobal op4 round(%iScen%+400)
$evalGlobal op5 round(%iScen%+700)


$setglobal scenWithoutIncgen  %scen%
$set       scen  incgen/%scen%

$else.iScen
```

In the case of normal single farm run, the standard option files will be
used.


 [^13]: This assures also under the restricted number of random values
    for each factor the components are still represented in a fully
    stratified manner over the entire range, each variable has the
    opportunity to show up as important, if it indeed is important
    (Iman, 2008).

 [^14]: This is necessary to restrict sampling time but also guarantees to
    find a random sample that appropriately implies the correlation
    structure as proposed by the user (more detailed explanation of this
    later in this paper)

 [^15]: Another possible routine for LHS sampling is
    "optimumLHS(\*\*\*)". But during our test runs it did not lead to
    more smooth space filling random draws, but increased the runtime of
    the sampling process. For optimal-LHS see also Park (1994).

# Technical Realization

!!!abstract
    The model uses GAMS for data transformations and model generation and applies the industry LP and MIP solver CPLEX for solution. The code adheres to strict coding guidelines, for instance with regard to naming conventions, code structuring and documentation, including a modular approach. A set of carefully chosen compilation and exploitation tests is used to check the code. The code is steered by a Graphical User Interface based on GGIG (ref., Java code) which also support result exploitation.

## Overview of the Technical Realization


The model template and the coefficient generator are realized in GAMS
(General Algebraic Modelling System), a widely used modelling language
for economic simulation models. GAMS is declarative (as seen from the
template discussion above), i.e. the structure of the model's equation
is declared once, and from there different model instances can be
generated. GAMS supports scripting for data transformation, extensively
used by the coefficient generator and by the post-model reporting.

11.Overview of technical realization

Source: Own illustration

Additionally, as an extension of the experiment exploiter, *machine
learning* (for detailed description see Britz, 2011) can
be used to derive correlations and dependencies between model results
and available model variables.

## MIP Solution Strategy


In opposition to purely linear problems, Mixed-Integer problem models
(MIPs) are far harder to solve. In order to find the optimum, in theory
the combinatorial set of all binaries respectively general integer
variables would need to be evaluated. Depending on the simulation
horizon of FARMDYN, the number of farm branches considered and the time
resolution for investment and labour use decisions, a model instance can
comprise between a few dozens to more than a thousand binary variables,
with often several ten thousands variables and equations in total.

There are huge differences in the quality of LP and more so MIP solvers.
Industry solvers such as CPLEX or GUROBI reflect continuous investments
into algorithmic improvements over decades. Fortunately, both offer free
academic licenses. The code is set-up to work with both solvers to be
secured should license conditions change as well as switch in cases one
of the solvers outperforms considerably the other. Current tests seem to
show a slight advantage for CPLEX. Both solvers can benefit from
parallel processing. Model instances should therefore if possible be
solved on a multi-core computing server. The option files for the
solvers are currently defined such that one core is not used by the
program and left free for other processing load.

The relaxed version of the model (where binaries and integers are
removed and treated as continuous variables) can typically be solved in
a few seconds, and once such a starting point is given, slight
modifications to the model take very little time to solve despite the
model size. However, regardless of tremendous algorithmic improvements
in solving MIPs, the MIP version could take quite long to solve without
some solution tactic.

The model code therefore integrates different strategies to speed up the
solution process for the MIP. Some of those are generally applicable to
MIP problems, typically offered by GAMS and/or the MIP solvers, others
follow tactics proposed to speed up the solution time of MIP problems,
but require a specific implementation reflecting the model structure. In
the following, these strategies are roughly described, starting first
with the model generic.

In order to define a lower bound on the objective which allows the
solver to cut-off parts of the tree, the model is first solved in
relaxed mode (RMIP) with the farm switched off such that income can only
generated by working off-farm (*v\_hasFarm* is fixed to zero). Solving
that variant takes less than a second. The solution is used to define
the lower cut-off for MIP solver. Next, the model is solved as RMIP with
only one state-of-nature, and afterwards, the state contingent variables
are copied to all other states-of-natures, before the RMIP is solved
again. The main statements (see *exp\_starter.gms* for details) are
shown below:

![P:\\Hiwi\\Schaub\\Sch√§fer\\Documentation\\Equation
ScreenS\\1.png](media/media/image236.png)

The relaxed (RMIP) solution defines the upper cut-off -- forcing certain
variables to only take on integer values can only reduce the objective
function. At the same time, it proves a basis for solving the MIP.
However, in many instances it has not proven useful to use the solution
of RMIP as MIP start starting point, both CPLEX and GUROBI seem to spend
considerable time to construct a feasible integer solution from the RMIP
solution.

As stated above, solving a MIP problem to its true optimum can be
tremendously time consuming. Therefore, typically MIP problems are only
solved given an optimality tolerance. The branch-and-cut algorithm used
in MIP solvers always provide a safe upper limit for the objective value
stemming from a relaxed version of the current tree node. Accordingly,
they can quantify the maximal absolute and relative gap to the
potentially maximal objective function. Typically, the smaller the
desired gap, the larger is number of combination of integer variables
the solver needs to test. Forcing the gap to zero requires more or less
a test of all combination, i.e. ten-thousands of solves of a LP version
of the model with binaries and integers fixed. In most production runs,
a relative gap of 0.5% has proven as acceptable. The solver will then
stop further search for a better solution once a MIP solution has been
found which differs by less from the relaxed best node.

The problem with the gap is clearly that differences between two
simulations can not only stem from different model inputs (prices,
policy etc.), but also simply from the fact that the gap at the best
solutions returned by the solver for each run differs.

MIP solvers can also "tune" their options based on one or several given
model instance. Tuning is available both with CPLEX and GUROBI, and can
be switched on via the interface. That process takes quite long, as the
model is repeatedly solved with different solver options. The parameters
from the tuning step are stored in an option and can be used by
subsequent runs.

### Fractional investments of machinery

An option to reduce the number of binaries is to treat certain
investment decisions as continuous. For machinery, the model allows to
replace the binary variable *v\_buyMach* by a fractional replacement
*v\_buyMachFlex*. The replacement depends on a threshold for the
depreciation costs per ha or hour, which can be set by the interface.
The larger the threshold, the lower is the number of integer variables
and the higher the (potential) difference to the solution where more
indivisibilities in machine investments are taken into account.

The relevant code section (*exp\_starter.gms*) is shown below:

![](/media/image237.png)

### Heuristic reduction of binaries

On demand, the RMIP solution can be used in combination with some
heuristic rules to reduce the set of endogenous variables. As the RMIP
solution will e.g. build a fraction of larger stables and thus save
costs compared to the MIP solution, the herd size in the MIP solution
can be assumed to be upper bounded by the solution of the MIP.
Similarly, as investment costs for machinery will be underestimated by
the MIP, it can be assumed that machinery not bought in the RMIP
solution will not be found in the optimal solution of the MIP.

An example is shown below for investment decision into stables. The
program first defines the maximal amount of stable places used in any
year. Investments into stables and their usage which are larger than the
maximal size or smaller then 2/3 of the maximal size are removed from
the MIP. Equally, investment in stables is set to zero if there was no
investment in the RMIP solution.

![](/media/image238.png)

Similar statements are available for investments into manure silos,
buildings and machinery. These heuristics are defined in
"*model\\reduce\_vars\_for\_mip.gms*". It is generally recommended to
use these statements as they can considerably reduce solving time.
However, especially after structural changes to the code, checks should
be done if the rules do not actually prevent the model from finding the
(optimal) MIP solution.

### Binary fixing heuristics

In order to speed up solution, the heuristics discussed above are
coupled with repeated RMIP solves where integer variable from the last
fractional solution are moved to zero or unity depending on the solution
and heuristics rules. To give an example: if parts of machinery are
bought over time such that their sum exceeds a threshold, for instance
half a tractor, the *machBuy* variable in the first year where the
machine is bought is fixed to zero. These pre-solves can lead to start
point for the MIP solves where most integer variables are already no
longer fractional which can speed up solution.

### Equations which support the MIP solution process

Another tactic to ease the solution of MIPs is to define equations,
which decrease the solution space for the integer variables based on the
level of fractional variables respectively define logical ordering for
the integer decisions. These equations are not necessarily truly
restricting the solution space, they only re-inforce existing relations
between variables. The additional equations often reduce the overall
solution time by improving the branching more then by increasing single
LP iterations due to the increase in the constraints.

One way to improve the branching order is to link binaries with regard
to dynamics. There are currently *three ordering equations over time*.
The first two prescribes that a farm respectively a cow herd in t+1
implies that a farm respectively a cow herd in the previous year
existed:

![](/media/image239.png)

![](/media/image240.png)

The second one implies that working off-farm in an year t implies also
working off-farm
afterwards:![](/media/image241.png)

Another tactic followed is to define logical high level binaries which
dominate other. These *general binaries* are partly already shown above:
the *v\_hasFarm* and *v\_workOffB* variables. The later one is linked to
the individual off-farm working possibilities:

![](/media/image242.png)

In order to support the solving process, *w\_workOff* is defined as a
*SOS1* variable, which implies that at most one of the *workType*
options is greater than zero in any year.

The *v\_hasFarm* variables dominates the *v\_hasBranch* variables:

![](/media/image243.png)

That equation is additionally linked to the logic of the model as
*v\_hasFarm* implies working hours for general farm management.

Furthermore, general binary exists which controls if a herd is present
in any year, *v\_hasAlwaysHerd*. If it is switched on, it will imply a
dairy herd in any year.This is based on the equation *hasAlwaysLast\_*
together with the order equation *hasHerdOrder\_* shown below.

![](/media/image244.png)

The equations which support the MIP solution process by linking
fractional variables to binary ones relate to investment decisions.
Firstly, investments in machinery are only possible if there is matching
machinery need:

![](/media/image245.png)Secondly, two equations link the dairy
herd to investment decisions into stables and manure storage silos:

![](/media/image246.png)

![](/media/image247.png)

These supporting restrictions can be switched off from the model via the
interface, to check if they unnecessarily restrict the solution domain
of the solver. It is generally recommended to use them as they have
proven to speed up the solution process.

### Priorities

Finally, there are options to help the MIP solver to decide which
branches to explore first. The variable field .prior in GAMS allows
setting priorities which are passed to the MIP solver; lower priorities
are interpreted as having precedence. The file "model\\def\_priors.gms"
defines such priorities.

The model is instructed to branch first on the decision to have a herd
in any year, next on having a farm and the individual branches:

![](/media/image248.png)

Generally, early years are given precedence:

![](/media/image249.png)

The *p\_priorMax* is the maximal priorities assigned to stables, which
is defined by a heuristic rule: large stables are tried before smaller
ones, cow stable before young cattle and calves stables, and finally
long term investment in the whole building done before maintenance
investments:

![](/media/image250.png)

Off-farm work decisions currently receive a lower priority compared to
investments into stables:

![](/media/image251.png)

For other investment decisions, the investment sum is used for priority
ordering:

![](/media/image252.png)

The SOS1 variables should have all the same priorities. Therefore, no
distinction is introduced for the v\_workOff and v\_siCovComb variables,
with the exemption of the time dimension.

Generally, it is recommend using these priorities as they have proven to
speed up the solution process.

## Reporting


As discussed in the following chapter, a GUI allows exploitation of
model results, also comparing different model runs. That part requires
that all results are stored in one multi-dimensional cube. Accordingly,
after the model is solved, its variables are copied to a result
parameter, as shown in the following example:

![](/media/image253.png)

## Systematic sensitivity analysis based on Design of Experiments


As discussed above, solution for one indicator and one GHG emission
target might require between a few seconds to several minutes on a
powerful multi-core machine. The derivation of the marginal abatement
cost curves requires solving repeatedly model instances over a range of
GHG emission targets, therefore it might require an hour or more to
solve one specific farm configuration.

An application of the model to a larger sample of existing farms is
consequently computationally impossible. This is why it was envisaged
from the beginning to use sensitivity analysis to generate a sufficient
number of instances to derive a meta-model in order to estimate
abatement costs for larger population of farms, for example based on an
appropriate regression model.. Meta modeling seems also a suitable tool
to learn more about which farm attribute impact abatement costs and to
which extend the occurring MACs depend on the GHG calculation procedure
of the different indicators.

For this four steps are required:

1.  Setting up of appropriate sensitivity experiments which cover the
    distribution of farm attributes in an appropriate sample (such as
    the farm structure survey for North-Rhine-Westphalia). Consequently,
    this requires the use of an efficient and space filling random
    sampling design to lower the necessary sample size for the
    derivation of a meta-model. At the same time it has to be ensured
    that the randomized factor level combinations are smoothly
    distributed over the range of factor level permutations. [^13]

2.  Running the single farm model on these experiments and collecting
    key results.

3.  Deriving a meta model from these experiments.

This section focuses mainly on technical aspects of this process.

The overall strategy consists of combining a Java based package for
interface generation and result exploitation, which also comprises a
machine learning package, with GAMS code. For the definition of
representative sensitivity experiments a sampling routine, (lhs\_0.10)
implemented in R (version 2.15.1) is combined with the GAMS code to
generate sample farms under recognition of correlations between factors.

The GAMS code *(scen\_gen*) modifies settings entered via the interface
(see next section) to define attributes for each experiment. A single
farm run is then executed as a child process with these settings. The
user is able to define upper and lower bounds for single factors to
define a solution room in which factor levels can vary between scenarios
for different production specific attributes of the farm (see next
section). The interface also allows defining if correlations between
selected variables should be recognized during the sample randomization
procedure. Furthermore, depending on the number of draws and the
complexity of the assumed correlation matrix a maximum number of
sampling repetitions can be selected [^14].

Only the factors for which the selected maximum value differs from the
minimum value are varied between model runs. Hence, the user is able to
fix factor levels for single factors over all experiments by defining
the minimum and maximum factor level. The upper and lower bounds of the
variables define the solution space of possible factor level
combinations of different factors. If the chosen minimum and maximum
values are equal, the factor level of the specific attribute is holding
constant during the scenario definitions. For the definition of wage
rates and prices for concentrates the user is able to select constant
differences to the full time wage rate or the concentrate type 1.

With increasing number of factors that can vary between scenarios and
increasing possible factor levels per factor, the number of possible
scenarios (factor level permutations) will increase exponentially (up to
a few thousands). Hence, to create model outputs representative for all
admissible scenarios, a large number of scenario runs would have to be
processed to get reliable outputs for the derivation of a meta-model.

As this would cause long computing time also on a multi-core processor
(several days), the numbers of scenario runs have to be restricted to a
manageable number, while at the same time being representative for the
real life distribution of farm attributes.

Therefore, the scenario definition is done by Latin Hypercube Sampling
(LHS) to create an efficient sample with a small sample size (to lower
computing time) while guaranteeing a space filling sample design over
the full range of admissible scenarios (McKay et al. 1979, Iman and
Conover 1980). This is done, using a bridge from GAMS to the statistical
software R. Therefore the LHS package of R has to be installed for being
able to create LHS samples for a defined number of draws *n* and factors
*k* (in our case taking the command "*improvedLHS(n,k*)"). LHS sampling
creates a sample matrix of size n\*k incorporating random values between
0 and 1, which are interpretable as percentages. These are drawn
assuming an uniform distribution between 0 and 1. Further on, LHS
sampling outputs ensure orthogonality of the output matrix and that
factor level combinations evenly distributed over the possible
permutation area.

The GAMS side of the technical implementation is shown in the following:

![](/media/image254.png)

The maximal run time for finding a sample can be defined, *maxRunTime.*
If correlations between variables are known and should be recognized
within the sampling prodedure, the command *useCorr has to be set to
"true"*. Then the correlation matrix can be defined specifically.

![](/media/image255.png)

The names of the set of varying factors, the factor names, the scenario
name, the desired number of draws and, if activated, also the
correlation matrix are send to R. Then the R file "*rbridge\\lhs.r*" is
executed.

![](/media/image256.png)

The R-bridge is hence activated (R side). Therefore several packages are
installed in R from the R library to be able to do LHS sampling:

![](/media/image257.png)

*p\_n* denotes the number of draws defined via the graphical user
interface, which is equivalent to the number of scenarios resulting from
the sampling routine. *Sys.getenv(....)* asks for commands or
information given by the environment (for example if correlations have
to be recognized or not).

![](/media/image258.png)

We decided to use the "*improvedLHS"* type for randomization [^15] which
produces a sample matrix of *n* rows and *k* columns (n = number of
draws, k = number of factors). This leads to a quite efficient sample
generation in R:

![](/media/image259.png)

Usually, input variables for sensitivity analysis in computer models are
assumed to be independent from each other (Iman et al., 1981a;b). Also
LHS sampling was designed to create a sample of factor level
combinations for different factors avoiding correlations between factors
in random draws to ensure a space filling output. But, for our purposes,
it is important to incorporate as much information about the
multivariate input distribution as possible to get more realistic sample
scenarios and exclude factor combinations that are rather impossible in
reality. Hence, following Iman and Conover (1982:p.331-332) correlation
structure information among input variables should be recognized within
the sampling process, if available. Otherwise "the theoretical
properties of the statistics formed from the output may no longer be
valid." (Iman and Conover 1982:p.331)

To also incorporate information about dependencies between interesting
variables during the sampling procedure we expanded the sampling method
by an approach of Iman and Conover (1982) designing a swapping algorithm
which shuffles observations for single factors between the draws to
mimic given *k\*k* correlation matrix (therefore the R package *MC2d*
including the routine *cornode* is necessary).

![](/media/image260.png)

To increase the possibility to randomize a sample which offers a
correlation matrix of factors near the proposed one, the routine allows
to repeat the random sampling of demanded *n* draws (yielding *n*
experiment scenarios) for a maximal given computing time ("*maxRunTime*"
e.g. 300 seconds.). The sample (incorporating *n* draws for *k* factors)
with the smallest mean percentage deviation (*meanDev*) between given
and randomized correlation matrix is then selected and send back to GAMS
as the random sample representing the possible population.
Alternatively, the repetition of *n* draws (*n* x *k* sampling matrix)
will be stopped by a threshold value (*if meanDev \< 1*) for the
deviation between the assumed and the randomized sample correlation
matrix.

![](/media/image261.png)

For the case that the correlations between factors are given by the
user, it leads to an undefined correlation matrix, the program adjusts
the correlation matrix to the nearest one possible:

![](/media/image262.png)

As mentioned above the LHS sampling defines random value combinations
between all factors in each single draw. Therefore uniform distributed
random values between 0 and 1 are drawn. The total set of draws defines
one random sample of n single experiments (factor level combinations (in
this stage of the sampling still between 0 and 1)). The routine
implemented into the LHS-module now tries to find the best fitting
sample which corresponds to the demanded correlation matrix most
properly. Sampling outputs of the LHS draws show efficiency
characteristics, also under recognition of correlations. This means that
the mean of drawn random values is still 0.5 (as LHS draws lie between 0
and 1). And if the number of draws is large enough (greater than 30),
quantiles as well as the mean of the distribution of LHS random values
show that we are still consistent with the assumption of an uniform
distribution function of the random draws (between 0 and 1), as
necessary for efficient LHS outputs, also under recognition of factor
correlations. The best fitting sample with the minimal average
percentage deviation of correlations between defined and randomized
correlation matrix is then selected and stored by the program and
automatically printed as a PDF-document for visualization. The PDF gives
also information about average percentage bias of the randomized
correlation matrix as well as the number of total draws which define the
number of resulting sample experiments:

![](/media/image263.png)

12.Scatterplot matrices for different LHS
    samples. With and without the recognition of factor correlations

On the left hand side one can see the scatter plot matrix without any
correlations between factors. In contrast, a clear difference in sampled
values is visualized by the right hand side matrix. For example a
correlation between *nCows* and *milkYield* was assumed to be 0.8. The
best fitting matrix lead to the same correlation between these two
factors. The correlation coefficients within brackets are the
correlations predefined by the operator. The values in front of the
brackets are the correlation coefficients fitted by the sampling matrix.
The average mean percentage deviation of the randomized correlation
matrix and the assumed correlation matrix is quantified by 7.34%,
meaning, that on average, the randomized correlations deviate by 7.34%
from the predefined ones. The distribution function in the diagonal
shows, that the sampled values of each factor still ensure a uniform
distribution.

The random values for the scenarios are transformed by GAMS to the real
factor levels following the distribution functions of single variables.
A uniform distribution of factor levels for the relevant variables is
assumed. These are easily to define by the minimal value *a* and the
maximum value *b*. A uniform distribution function can be defined by the
following density function (left graph):

(1) $f\left( x \right) = \frac,a \leq b$

Values below *a*, or above *b* have a probability of 0. The
antiderivative expresses the cumulative distribution function of the
random variable whose values lie within the interval \[0; 1\] (right
graph):

(2) $F\left( x \right) = \frac,\ a \leq x \leq b$

![](/media/image264.png)

13.Density function and cumulative
    distribution function of an uniform distributed variable

From the left hand side density function one can easily derive the right
hand side cumulative distribution function. The *y* value of the
distribution function equals the integral
$\int_^$ below the density function (cumulative
probabilities below x).

The given random values of the R-routine (*F(x)*) enable the allocation
of corresponding factor levels (between *a* and *b*) to this random
percentage values from the cumulative distribution function. A random
cumulative probability value *y* corresponds to the factor level *x*
which lies within the real value domain of the interesting factor. Hence
this random sampling procedure produces random values by transforming
uniform distributed random percentages (between 0 and 1) to factor
levels, which are conform to the assumed distribution function of the
variable. So far a uniform distribution function is assumed for the real
factor levels. (this can be adjusted to other functions if for example a
known population has to be simulated).

For an assumed uniform distribution function of factor levels this is
done following the formula:

(3) $F\left( x \right)*\left( b - a \right) + a = x$

The randomized value *y* is transformed to the factor level room
concerning the given distribution function of the factor. Hence for each
single random draw a value is generated for the interesting variable
corresponding to its assumed probability distribution. If a different
distribution is assumed, formula 3 changes.

In GAMS code the formula (3) has to be applied for each factor to
calculate the sample values whereat *p\_doe(draws,"factor")* is
equivalent to F(x). The random percentage *p\_doe(\*,\*)* has to be
multiplied by the difference between the possible maximum and minimum
value of the factor *(%factorMax% - %factorMin%)*. Afterwards the min
value *(%factorMin%)* has to be added to the product to yield the factor
level *x* for the specific factor and scenario. This is illustrated for
some parameters in the following.

![](/media/image265.png)

p\_scenParam(draws,factor) gives the scenario parameter of one factor
defined by the random values given by the LHS sampling routine. The
combination of factor levels of the different factors for one single
draw defines one single sensitivity scenario.

The set *scenItems* defines which settings are (possibly) defined
specific for each scenario:

![](/media/image266.png)

Nevertheless correlations between factors are able to be recognized
during the sample generation to avoid factor level combinations within
scenarios that conflict with common statistical knowledge; the model
code enables the user to specifically exclude factor level combinations
that seem to be implausible. For example high labor input per cow and
low milk yield levels or high numbers of cows per farm and only very low
yielding phenotypes.

![](/media/image267.png)

These scenario settings must be stored in a GAMS file which is then
picked up by the child processes. In order to keep the system
extendable, firstly, all settings inputted via the Graphical User
Interface are copied over to the specific scenario:

![](/media/image268.png)

Secondly, the modifications defining the specific sensitivity
experiment, i.e. the scenario, are appended with GAMS file output
commands (see scenGen\\gen\_inc\_file.gms):

![](/media/image269.png)

Finally, the content is copied to a specific scenario input file:

![](/media/image270.png)

The code to build and optimize the single farm model itself is realized
in GAMS and uses CPLEX 12.6 in parallel mode as the MIP solver.
Automatic tuning is used to let CPLEX use appropriate solver setting on
the problem. The model instances are set up such as to avoid any
conflicts with I/O operations to allow for parallel execution.

A single instance has a typical load of about 1.8 cores in average. In a
multi-core machine it seems promising to execute several such processes
in parallel. That is realized by a GAMS program which starts each model
on its own set of input parameters:

![](/media/image271.png)

The name of the scenario, *allScen.tl* is passed as an argument to the
process which will lead a specific include file comprises the definition
of the scenario.

The GAMS process will use its own commando processors and run
asynchronously to the GAMS thread which has started it. The calling
mother process has to wait until all child processes have terminated.
That is achieved by generating a child process specific flag file before
starting the child process:

![](/media/image272.png)

This flag file will be deleted by the child process when it finalizes:

![](/media/image273.png)

A simple DOS script waits until all flags are deleted:

![](/media/image274.png)

Using that set-up would spawn for each scenario a GAMS process which
would then execute all in parallel. The mother process would wait until
all child processes have deleted their flag files before collecting
their results. As several dozen or even hundredth of scenarios might be
executed, that might easily block the machine completely, e.g. by
exceeding the available memory.

It is hence necessary to expand the set-up by a mechanism which ensures
that only a pre-defined number of child processes is active in parallel.
That is established by a second simple DOS script which waits until the
number of flag files drops below a predefined threshold:

![](/media/image275.png)

Finally, the results from individual runs are collected and stored. A
GAMS facility is used to define the name of a GDX file to read at run
time:

![](/media/image276.png)

And load from there the results of interest:

We now transformed all MAC estimates which are 0 due to an exit decision
of a farm to be able to select these cases for our meta-modelling
estimation (Heckman two-stage selection, described in the next technical
documentation: "R routine to estimate Heckman two stage regression
procedure on marginal abatement costs of dairy farms, based on large
scale outputs of the model DAIRYDYN" by Britz and Lengers (2012)).

![](/media/image277.png)

![](/media/image278.png)

Further on, the scenario specific settings which can be used as
explanatory variables for later regressions are stored:

![](/media/image279.png)

In a next step, the results are stored in a GDX container

![](/media/image280.png)

The major challenge consists in ensuring that the child processes do not
execute write operation on shared files. In the given example, that
relates to the GAMS listing, the GDX container with the results and the
option files generated by the CPLEX tuning step. For the latter, two
options are available: (1) set up child specific directory, copy the
option files into it and use the *optdir* setting in GAMS, or (2) label
the option files accordingly. That latter option was chosen which
restricts the number of scenarios to 450:

![](/media/image281.png)

In the case of normal single farm run, the standard option files will be
used.


 [^13]: This assures also under the restricted number of random values
    for each factor the components are still represented in a fully
    stratified manner over the entire range, each variable has the
    opportunity to show up as important, if it indeed is important
    (Iman, 2008).

 [^14]: This is necessary to restrict sampling time but also guarantee to
    find a random sample that appropriately implies the correlation
    structure as proposed by the user (more detailed explanation of this
    later in this paper

 [^15]: Another possible routine for LHS sampling is
    "optimumLHS(\*\*\*)". But during our test runs it did not lead to
    more smooth space filling random draws, but increased the runtime of
    the sampling process. For optimal-LHS see also Park (1994).
